{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "2a074b4b",
            "metadata": {
                "editable": true,
                "origin_pos": 0,
                "slideshow": {
                    "slide_type": "slide"
                },
                "tags": []
            },
            "source": [
                "# 4.1. Softmax Regression\n",
                "\n",
                "**Regression**: How much? or How many?\n",
                "\n",
                "**Classification**: Which category does this belong to?\n",
                "\n",
                "Examples of classification:\n",
                "- Email: spam or not spam\n",
                "- Image: cat, dog, or bird\n",
                "- Handwriting: digit 0-9\n",
                "- Recommendation: which movie to watch next"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d365a694",
            "metadata": {
                "editable": true,
                "jp-MarkdownHeadingCollapsed": true,
                "slideshow": {
                    "slide_type": "slide"
                },
                "tags": []
            },
            "source": [
                "## 1. Classification\n",
                "\n",
                "Suppose we have a collection of 2x2 pixel images (grayscale), and we want to classify them into 3 categories: dog, cat, or chicken.\n",
                "\n",
                "- Features: $\\mathbf{x}=(x_1, x_2, x_3, x_4)$ (2x2 pixel values)\n",
                "- Labels: $y \\in \\{dog, cat, chicken\\}$\n",
                "\n",
                "How to represent the labels?\n",
                "1. $y \\in \\{0, 1, 2\\}$: dog=0, cat=1, chicken=2\n",
                "   - This is called **integer encoding**\n",
                "   - This is not a good representation for classification\n",
                "   - Because the model may think that dog < cat < chicken\n",
                "   - Might be useful for ordinal regression (e.g., rating 1-5)\n",
                "2. **One-hot encoding**: $y \\in \\{(1, 0, 0), (0, 1, 0), (0, 0, 1)\\}$\n",
                "   - dog=(1, 0, 0), cat=(0, 1, 0), chicken=(0, 0, 1)\n",
                "   - This is a better representation for classification\n",
                "   - Each category is represented by a vector with one element as 1 and the rest as 0\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ea8f849b",
            "metadata": {},
            "source": [
                "### 1.1. Linear Model\n",
                "![alt text](https://d2l.ai/_images/softmaxreg.svg)\n",
                "\n",
                "*Figure 1: Visualization of the softmax regression model.*\n",
                "\n",
                "$ \\mathbf{o} = \\mathbf{W} \\mathbf{x} + \\mathbf{b} $\n",
                "- $\\mathbf{o} \\in \\mathbb{R}^{3}$: output vector\n",
                "- $\\mathbf{W} \\in \\mathbb{R}^{3\\times4}$: weight matrix (size: 3x4) \n",
                "- $\\mathbf{b} \\in \\mathbb{R}^{3}$: bias vector (size: 3x1)\n",
                "- $\\mathbf{x} \\in \\mathbb{R}^{4}$: input vector (size: 4x1)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "18a628e6",
            "metadata": {},
            "source": [
                "### 1.2. The Softmax Function\n",
                "\n",
                "\n",
                "We could just train a linear model and use the output as the predicted class. \n",
                "\n",
                "However, this would not be a good idea because **the output could be negative or greater than 1**. This is not a valid probability distribution.\n",
                "\n",
                "We need to convert the output into a **probability distribution** (0~1).\n",
                "\n",
                "#### Probit model (obsolete)\n",
                "\n",
                "- Label = Output + Gaussian noise\n",
                "- $\\mathbf{y}=\\mathbf{o}+\\epsilon$, where $\\epsilon$ is a Gaussian noise.\n",
                "- Does not work well in practice.\n",
                "\n",
                "#### Softmax function\n",
                "- Idea: $P(y=i)\\propto \\exp(o_i)$ and normalize it.\n",
                "\n",
                "$$\n",
                "\\mathbf{\\hat{y}} = \\text{softmax}(\\mathbf{o})\n",
                "\\quad \\text{where} \\quad \\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j\\exp(o_j)}\n",
                "$$\n",
                "\n",
                "- Then, we can interpret $\\hat{y}_i$ as the probability of class $i$.\n",
                "\n",
                "- Also, the softmax preserves the order of the output.\n",
                "  - If $o_i > o_j$, then $\\hat{y}_i > \\hat{y}_j$.\n",
                "  - If $o_i < o_j$, then $\\hat{y}_i < \\hat{y}_j$.\n",
                "  - If $o_i = o_j$, then $\\hat{y}_i = \\hat{y}_j$.\n",
                "  - $ \\argmax_i \\hat{y}_i = \\argmax_i o_i $\n",
                "\n",
                "- Idea of softmax is from the **Boltzmann distribution**.\n",
                "- Actually, one may introduce a temperature parameter $T$ to control the sharpness of the distribution."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "14c650d9",
            "metadata": {},
            "source": [
                "### 1.3. Vectorization of Softmax\n",
                "\n",
                "- The softmax function can be vectorized as follows:\n",
                "$$\n",
                "\\begin{aligned}\n",
                "& \\mathbf{O} = \\mathbf{X} \\mathbf{W} + \\mathbf{b} \\\\\n",
                "& \\mathbf{\\hat{Y}} = \\text{softmax}(\\mathbf{O}) \n",
                "\\end{aligned}\n",
                "$$\n",
                "\n",
                "- $\\mathbf{O} \\in \\mathbb{R}^{n\\times q}$: output matrix\n",
                "- $\\mathbf{X} \\in \\mathbb{R}^{n\\times d}$: input matrix (row-wise)\n",
                "- $\\mathbf{W} \\in \\mathbb{R}^{d\\times q}$: weight matrix\n",
                "- $\\mathbf{b} \\in \\mathbb{R}^{q}$: bias vector"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e2619ee8",
            "metadata": {},
            "source": [
                "## 2. Loss Function\n",
                "\n",
                "Our linear model maps features $\\mathbf{x}$ to probabilities $\\hat{\\mathbf{y}}$.\n",
                "\n",
                "$\\mathbf{\\hat{y}}$ can be interpreted as the estimated conditional probability of each class given the input $\\mathbf{x}$:\n",
                "\n",
                "$$\n",
                "\\hat{y}_i = \\frac{\\exp(o_i)}{\\sum_j\\exp(o_j)} = P(y=i|\\mathbf{x}) \n",
                "$$\n",
                "\n",
                "If we use one-hot encoding for the labels, we can represent the label as a vector $\\mathbf{y} \\in \\mathbb{R}^{q}$, where $y_j=1$ if $j$ is the true class and $y_j=0$ otherwise.\n",
                "\n",
                "We can train the model by maximizing the likelihood function:\n",
                "\n",
                "$$\n",
                "P(\\mathbf{Y}|\\mathbf{X}) = \\prod_{i=1}^n P(\\mathbf{y}^{(i)}|\\mathbf{x}^{(i)})\n",
                "$$\n",
                "\n",
                "(Assuming independence: each sample is independently drawn from the same distribution.)\n",
                "\n",
                "We usually convert the max. likelihood to the min. negative **log-likelihood**:\n",
                "\n",
                "$$\n",
                "-\\log P(\\mathbf{Y}|\\mathbf{X}) = -\\sum_{i=1}^n \\log P(\\mathbf{y}^{(i)}|\\mathbf{x}^{(i)})\n",
                "=-\\sum_{i=1}^n l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)})\n",
                "$$\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2909f1f7",
            "metadata": {},
            "source": [
                "Then, how do we compute $l(\\mathbf{y}^{(i)}, \\hat{\\mathbf{y}}^{(i)})$?\n",
                "\n",
                "The **cross-entropy loss** function is commonly used.\n",
                "\n",
                "$$l(\\mathbf{y}, \\hat{\\mathbf{y}})=-\\sum_{j=1}^q y_j \\log(\\hat{y}_j)$$\n",
                "\n",
                "- Cross entropy loss is a measure of the difference between two probability distributions: the true distribution $\\mathbf{y}$ and the predicted distribution $\\hat{\\mathbf{y}}$.\n",
                "\n",
                "- How to calculate $l(\\mathbf{y}, \\hat{\\mathbf{y}})$:\n",
                "\n",
                "$$\n",
                "\\begin{aligned}\n",
                "l(\\mathbf{y}, \\hat{\\mathbf{y}}) &= -\\sum_{j=1}^q y_j \\log(\\hat{y}_j) \\\\\n",
                "&= -\\sum_{j=1}^{q} y_j \\log(\\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)}) \\\\\n",
                "&= \\sum_{j=1}^{q} y_j \\log(\\sum_{k=1}^q \\exp(o_k)) - \\sum_{j=1}^{q} y_j o_j \\\\\n",
                "&= \\log(\\sum_{k=1}^q \\exp(o_k)) - \\sum_{j=1}^{q} y_j o_j \\\\\n",
                "\\end{aligned}\n",
                "$$\n",
                "\n",
                "The gradient is\n",
                "\n",
                "$$\n",
                "\\begin{aligned}\n",
                "\\frac{\\partial l(\\mathbf{y}, \\hat{\\mathbf{y}})}{\\partial o_j} &= \\frac{\\partial}{\\partial o_j} \\left( \\log(\\sum_{k=1}^q \\exp(o_k)) - \\sum_{k=1}^{q} y_k o_k \\right) \\\\\n",
                "&= \\frac{\\partial}{\\partial o_j} \\log(\\sum_{k=1}^q \\exp(o_k)) - y_j \\\\\n",
                "&= \\frac{\\exp(o_j)}{\\sum_{k=1}^q \\exp(o_k)} - y_j \\\\\n",
                "&= \\hat{y}_j - y_j \\\\\n",
                "&= \\text{softmax}(\\mathbf{o})_j - y_j\n",
                "\\end{aligned}\n",
                "$$\n",
                "\n",
                "The derivative of the loss function with respect to the output $o_j$ is simply the difference between the predicted probability $\\hat{y}_j$ and the true label $y_j$.\n",
                "(This is a generic result for any exponential family distribution.)\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        },
        "required_libs": []
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
